{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exercícios spaCy\n",
        "\n",
        "**Objetivo:**  \n",
        "Exercicios em aula para conhecer mais da ferramenta spaCy.\n",
        "\n",
        "**Bibliotecas:**\n",
        "- NLTK\n",
        "- Pandas\n",
        "- spaCy"
      ],
      "metadata": {
        "id": "1fXGAB_lGzFr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importando bibliotecas e baixando spaCy"
      ],
      "metadata": {
        "id": "nYcPSrZ2QIKT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQXj3ukf6WbH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acedc41a-7fd2-4b2c-e820-ec97db1eb74f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pt-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.8.0/pt_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pt-core-news-sm\n",
            "Successfully installed pt-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download pt_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "import pandas as pd\n",
        "import nltk"
      ],
      "metadata": {
        "id": "OjunCkNU94wA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pln = spacy.load('pt_core_news_sm')"
      ],
      "metadata": {
        "id": "6MCm7NeI-HTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) Verifique versão, carregamento e pipes\n",
        "\n",
        "Objetivo: Checar ambiente e visão geral do pipeline.\n",
        "\n",
        "Tarefas:\n",
        "- Imprima a versão do spaCy.\n",
        "- Carregue o modelo em PT-BR e mostre os pipes com nlp.pipe_names.\n",
        "- Texto de teste: “Hoje estudarei Processamento de Linguagem Natural na faculdade.”"
      ],
      "metadata": {
        "id": "IgRXRQR3ApoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(spacy.__version__)\n",
        "print(pln.pipe_names)\n",
        "teste = pln('Hoje estudarei Processamento de Linguagem Natural na faculdade.')\n",
        "print(teste)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HW7J5mc-s0G",
        "outputId": "bdea4b5d-757e-4f67-94c5-fdf52d1dba5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.8.11\n",
            "['tok2vec', 'morphologizer', 'parser', 'lemmatizer', 'attribute_ruler', 'ner']\n",
            "Hoje estudarei Processamento de Linguagem Natural na faculdade.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) Tokenização e POS\n",
        "\n",
        "Objetivo: Observar como o spaCy separa tokens e atribui classes gramaticais.\n",
        "\n",
        "Tarefas:\n",
        "- Tokenize e liste (token.text, token.pos_).\n",
        "- Texto: “O professor apresentou exemplos claros na aula de PLN.”"
      ],
      "metadata": {
        "id": "ySUiczk1AyGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto2 = pln('O professor apresnetou exemplos claros na aula de PLN.')\n",
        "\n",
        "for token in texto2:\n",
        "  print(token.text, token.pos_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bNm9QJ9A18A",
        "outputId": "47a66884-3481-466f-d42f-4f994a7152d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O DET\n",
            "professor NOUN\n",
            "apresnetou VERB\n",
            "exemplos NOUN\n",
            "claros ADJ\n",
            "na ADP\n",
            "aula NOUN\n",
            "de ADP\n",
            "PLN PROPN\n",
            ". PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) Filtre nomes próprios\n",
        "\n",
        "Objetivo: Praticar filtragem por categoria POS.\n",
        "\n",
        "Tarefas:\n",
        "- Liste apenas tokens com pos_ == 'PROPN'.\n",
        "- Texto: “Ana viajou com Roberto para Florianópolis durante o feriado.”"
      ],
      "metadata": {
        "id": "PAbRs5lJB3pM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto3 = pln('Ana viajou com Roberto para Florianópolis durante o feriado.')\n",
        "\n",
        "for token in texto3:\n",
        "  if token.pos_ == 'PROPN':\n",
        "    print(token.text, token.pos_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGc164AbB9dy",
        "outputId": "b371149b-01ea-4f70-ab6e-071ab62a3829"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ana PROPN\n",
            "Roberto PROPN\n",
            "Florianópolis PROPN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4) Lematização\n",
        "\n",
        "Objetivo: Comparar formas flexionadas e lemas.\n",
        "\n",
        "Tarefas:\n",
        "- Para cada token, imprima (token.text, token.lemma_).\n",
        "- Texto: “compramos compraria compraram comprarão comprando”"
      ],
      "metadata": {
        "id": "YKCve6ZjCq38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto4 = pln('compramos compraria compraram comprarão comprando')\n",
        "\n",
        "for token in texto4:\n",
        "  print(token.text, token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxvKbDLpCxPx",
        "outputId": "225b30e4-a2ff-4673-c203-3ee49b4ceebc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compramos compramos\n",
            "compraria compraria\n",
            "compraram comprar\n",
            "comprarão comprar\n",
            "comprando comprar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5) Stopwords\n",
        "\n",
        "Objetivo: Praticar remoção de palavras de função.\n",
        "\n",
        "Tarefas:\n",
        "- Remova tokens que são stopwords e reconstrua a frase “limpa”.\n",
        "- Texto: “Eu não tenho nada contra, mas às vezes é melhor esperar.”"
      ],
      "metadata": {
        "id": "nn-6VB0WDWjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto5 = pln('Eu não tenho nada contra, mas às vezes é melhor esperar.')\n",
        "\n",
        "for token in texto5:\n",
        "  if not token.is_stop:\n",
        "    print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFM8PWPwDaQv",
        "outputId": "84f31783-648e-4d49-efb5-4c4e231845c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ",\n",
            "melhor\n",
            "esperar\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6) NER básico + visualização\n",
        "\n",
        "Objetivo: Reconhecer entidades nomeadas em PT-BR.\n",
        "\n",
        "Tarefas:\n",
        "- Extraia entidades (ent.text, ent.label_).\n",
        "- Opcional: visualize com displacy.render(doc, style='ent', jupyter=True).\n",
        "- Texto: “A Apple lançou produtos em Cupertino em 2024 e planeja eventos no Brasil.”"
      ],
      "metadata": {
        "id": "TeQZJKqAEg7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto6 = pln('A Apple lançou produtos em Cupertino em 2024 e planeja eventos no Brasil.')\n",
        "\n",
        "for entidade in texto6.ents:\n",
        "  print(entidade.text, entidade.label_)\n",
        "\n",
        "displacy.render(texto6, style='ent', jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "q-pIq7atEpFE",
        "outputId": "a9d95208-a78c-4e6e-eb30-932e589eb561"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple ORG\n",
            "Cupertino LOC\n",
            "Brasil LOC\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">A \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Apple\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " lançou produtos em \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Cupertino\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              " em 2024 e planeja eventos no \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Brasil\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              ".</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7) Similaridade entre palavras\n",
        "\n",
        "Objetivo: Investigar proximidade semântica entre palavras.\n",
        "\n",
        "Tarefas:\n",
        "- Compare pares de tokens com token1.similarity(token2).\n",
        "- Pares sugeridos: (“feliz”, “contente”), (“feliz”, “triste”), (“triste”, “deprimido”)."
      ],
      "metadata": {
        "id": "ROqbggeyFgPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p1 = pln('feliz')\n",
        "p2 = pln('contente')\n",
        "p3 = pln('triste')\n",
        "p4 = pln('deprimido')\n",
        "\n",
        "print(p1.similarity(p2))\n",
        "print(p1.similarity(p3))\n",
        "print(p3.similarity(p4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwfW47NSFj0-",
        "outputId": "056ace29-ef07-4fea-896f-39327b7a93c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5661238431930542\n",
            "0.14896716177463531\n",
            "-0.030418669804930687\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3466372568.py:6: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print(p1.similarity(p2))\n",
            "/tmp/ipython-input-3466372568.py:7: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print(p1.similarity(p3))\n",
            "/tmp/ipython-input-3466372568.py:8: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print(p3.similarity(p4))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8) Lema vs. Stem (RSLP)\n",
        "\n",
        "Objetivo: Entender diferenças entre lematização e stemming para PT-BR.\n",
        "\n",
        "Tarefas:\n",
        "- Monte um DataFrame com colunas: palavra, lemma_spacy, stem_rslp (NLTK).\n",
        "- Use um conjunto misto: ['organização', 'organizar', 'organizado', 'analista', 'analisar', 'análise', 'fácil', 'facilmente'].\n",
        "- Comente 3 casos em que stem e lema divergem e o impacto na análise"
      ],
      "metadata": {
        "id": "aQ0gWw5NHNox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('rslp')\n",
        "stemmer = nltk.stem.RSLPStemmer()\n",
        "conjunto = ['andando', 'organizar', 'organizado', 'analista', 'analisar', 'análise', 'fácil', 'facilmente']\n",
        "comparacao = []"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jauHj_RdHSfo",
        "outputId": "c514b796-72ae-4b33-deed-d0ccce7c9905"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping stemmers/rslp.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in conjunto:\n",
        "  token = pln(token)[0]\n",
        "  palavra = token.text\n",
        "  lematizacao = token.lemma_\n",
        "  stemizacao = stemmer.stem(token.text)\n",
        "  comparacao.append([palavra, lematizacao, stemizacao])\n",
        "\n",
        "  df = pd.DataFrame(comparacao, columns=['palavra', 'lemma_spacy', 'stem_rslp (nltk)'])\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYP4GWA2PEUr",
        "outputId": "1c34b15a-42b7-49c7-db81-9adb2af0a2f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      palavra lemma_spacy stem_rslp (nltk)\n",
            "0     andando       andar              and\n",
            "1   organizar   organizar          organiz\n",
            "2  organizado   organizar          organiz\n",
            "3    analista    analista             anal\n",
            "4    analisar    analisar           analis\n",
            "5     análise    análiser           anális\n",
            "6       fácil       fácil            fácil\n",
            "7  facilmente  facilmente            facil\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Comente 3 casos em que stem e lema divergem e o impacto na análise\n",
        "\n",
        "no caso de organizar o lema mantem a palavra organizar enquanto o stem reduz a palavra pra organiz que não é uma palavra completa, isso pode reduzir a precisão de uma analise semântica.\n",
        "\n",
        "Isso tambem acontece na palavra analisar que tambem é mantida no lemma e é reduzida para analis no stem. Usar o stem pode causar uma perda de informação já que a redução da palavra pode gerar ambiguidades\n",
        "\n",
        "Facilmente segue o mesmo padrão dos anteriores e permanece igual no lema mas é reduzida para facil no stem."
      ],
      "metadata": {
        "id": "9gHAbtWZPtFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9) Dependências: origem e destino\n",
        "\n",
        "Objetivo: Usar relações de dependência para extrair slots (origem/destino).\n",
        "\n",
        "Tarefas:\n",
        "- Identifique origem e destino sem usar índices fixos (use heads, children e preposições).\n",
        "- Texto: “Reserve passagens saindo de Congonhas e chegando em Recife amanhã cedo.”"
      ],
      "metadata": {
        "id": "qm-7Z0lwTggi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto9 = pln('Reserve passagens saindo de Congonhas e chegando em Recife amanhã cedo.')\n",
        "\n",
        "for entidade in texto9.ents:\n",
        "  for token in entidade:\n",
        "    if token.head.text == 'saindo':\n",
        "      print(f\"Origem: {token.text}\")\n",
        "    if token.head.text == 'chegando':\n",
        "      print(f\"Destino: {token.text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PU2s4wlgTkny",
        "outputId": "f70d0081-8e48-4068-b7ef-96f32cda1cb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Origem: Congonhas\n",
            "Destino: Recife\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10) Mapear objeto → local via dependências\n",
        "\n",
        "Objetivo: Praticar navegação em dependências para relacionar substantivos.\n",
        "\n",
        "Tarefas:\n",
        "- Associe o objeto à localização com base em suas relações.\n",
        "- Texto: “Precisamos de uma mesa para o restaurante e de um quarto para o hotel.”\n",
        "- Imprima pares: mesa→restaurante, quarto→hotel."
      ],
      "metadata": {
        "id": "vHqtAbXdYvqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto10 = pln('Precisamos de uma mesa para o restaurante e de um quarto para o hotel.')\n",
        "\n",
        "lugares = texto10[6], texto10[13]\n",
        "objetos = texto10[3], texto10[10]\n",
        "lugares, objetos\n",
        "\n",
        "for local in lugares:\n",
        "  for objeto in local.ancestors:\n",
        "    if objeto in objetos:\n",
        "      print(\"{} -> {}\".format(objeto, local))\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WsxYFs0Y23R",
        "outputId": "d5d71d6f-091f-4bf4-e8a6-fe669b357d6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mesa -> restaurante\n",
            "quarto -> hotel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11) Ações vinculadas a lugares\n",
        "\n",
        "Objetivo: Generalizar padrão de ações ligadas a locais.\n",
        "\n",
        "Tarefas:\n",
        "- Extraia pares lugar → verbo associado (ancestors/head).\n",
        "- Texto: “Quais museus podemos visitar em Lisboa e o que podemos conhecer em Sintra?”"
      ],
      "metadata": {
        "id": "LjNdAlydMj17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto11 = pln('Quais museus podemos visitar em Lisboa e o que podemos conhecer em Sintra?')\n",
        "\n",
        "lugares = texto11[5], texto11[12]\n",
        "verbo = texto11[3], texto11[10]\n",
        "lugares, verbo\n",
        "\n",
        "for local in lugares:\n",
        "  for acao in local.ancestors:\n",
        "    if acao in verbo:\n",
        "      print(\"O verbo {} esta associado a {}\".format(acao, local))\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6M7DKF7rMtYo",
        "outputId": "2dba6c7a-062b-4f68-a413-26bc2eb040c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O verbo visitar esta associado a Lisboa\n",
            "O verbo conhecer esta associado a Sintra\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12) Similaridade de sentenças com ranking\n",
        "\n",
        "Objetivo: Comparar semântica de sentenças curtas.\n",
        "\n",
        "Tarefas:\n",
        "- Dado o conjunto A, B, C, calcule similaridades A↔B e A↔C e explique resultados.\n",
        "- A: “Quando será inaugurado o novo campus?”\n",
        "- B: “O novo campus será inaugurado no próximo semestre.”\n",
        "- C: “Quantas vagas restaram para o curso noturno?”"
      ],
      "metadata": {
        "id": "wNvfUU61PK5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = pln('Quando será inaugurado o novo campus?')\n",
        "b = pln('O novo campus será inaugurado no próximo semestre.')\n",
        "c = pln('Quantas vagas restaram para o curso noturno?')\n",
        "\n",
        "print(f'A similaridade do texto A com o texto B é {a.similarity(b)}')\n",
        "print(f'A similaridade do texto A com o texto C é {a.similarity(c)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bT0xnOqXPQXN",
        "outputId": "f1059cf1-9471-4eee-bb16-3aa7e1fc7bf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A similaridade do texto A com o texto B é 0.8205576539039612\n",
            "A similaridade do texto A com o texto C é 0.4572163224220276\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1721798225.py:5: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print(f'A similaridade do texto A com o texto B é {a.similarity(b)}')\n",
            "/tmp/ipython-input-1721798225.py:6: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print(f'A similaridade do texto A com o texto C é {a.similarity(c)}')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 13) Stopwords personalizadas (negadores e domínio)\n",
        "\n",
        "Objetivo: Ajustar vocabulário às necessidades da tarefa.\n",
        "\n",
        "Tarefas:\n",
        "- Adicione 'curso' e 'prova' às stopwords e remova 'não' e 'nunca'.\n",
        "- Reaplique em: “Eu não vou faltar à prova do curso, nunca!”\n",
        "- Discuta como preservar negação influencia análises de sentimento."
      ],
      "metadata": {
        "id": "APfv0xoCQPr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto13 = pln('Eu não vou faltar à prova do curso, nunca!')\n",
        "\n",
        "pln.vocab[\"curso\"].is_stop = True\n",
        "pln.vocab[\"prova\"].is_stop = True\n",
        "pln.vocab[\"não\"].is_stop = False\n",
        "pln.vocab[\"nunca\"].is_stop = False\n",
        "\n",
        "print('Palavras que são stop words')\n",
        "for token in texto13:\n",
        "  if token.is_stop:\n",
        "    print(token.text)\n",
        "\n",
        "print('\\nPalavras que não são stop words')\n",
        "for token in texto13:\n",
        "  if not token.is_stop:\n",
        "    print(token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8K1QpLUYVpse",
        "outputId": "078d71ea-fdad-45fd-b3ab-463dd61d39c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palavras que são stop words\n",
            "Eu\n",
            "à\n",
            "prova\n",
            "do\n",
            "curso\n",
            "\n",
            "Palavras que não são stop words\n",
            "não\n",
            "vou\n",
            "faltar\n",
            ",\n",
            "nunca\n",
            "!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 14) Tokenização de casos especiais + limpeza com .is_right_punct\n",
        "\n",
        "Objetivo: Analisar splits e aplicar filtro com .is_right_punct no pré-processamento.\n",
        "\n",
        "Tarefas:\n",
        "- Tokenize e liste tokens obtidos.\n",
        "- Texto: “Promoção: e-commerce com frete grátis por R$ 1.299,90 — aproveite já!”\n",
        "- Remova tokens de pontuação à direita usando token.is_right_punct e reconstrua o texto limpo.\n",
        "- Comente se a tokenização ajuda ou atrapalha a extração de preço."
      ],
      "metadata": {
        "id": "njnb3ysqY3C9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto14 = pln('Promoção: e-commerce com frete grátis por R$ 1.299,90 — aproveite já!')\n",
        "sem_pontuacao = []\n",
        "\n",
        "for token in texto14:\n",
        "  if not token.is_punct:\n",
        "    sem_pontuacao.append(token.text)\n",
        "\n",
        "print(' '.join(sem_pontuacao))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiZM1BJCZAHe",
        "outputId": "26ffd1ff-b384-4d5d-f74f-1454d9cf9f64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Promoção e-commerce com frete grátis por R$ 1.299,90 aproveite já\n"
          ]
        }
      ]
    }
  ]
}